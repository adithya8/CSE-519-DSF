# -*- coding: utf-8 -*-
"""HW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wg8LczdDTYbOI8v78CzX03x5_1khsA7A

# Homework 2 - IEEE Fraud Detection

# Import statements and data loading
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd 
import numpy as np
import plotly.graph_objects as go
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from plotly.offline import init_notebook_mode, iplot
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
from collections import Counter 
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/My\ Drive/CSE 519/HW2

# these features were decided based on the feature importance scores after the initial iteration of the model. 
transCol = ['isFraud', 'TransactionID', 'C1', 'card1', 'TransactionAmt', 'card2', 'C7', 'C13', 'C14', 'P_emaildomain', 'D15', 'card5', 
       'C2', 'dist1', 'R_emaildomain', 'M1', 'ProductCD', 'card6', 'card4', 'TransactionDT', 'dist2', 'addr1', 'addr2']

idCol = ['id_02', 'DeviceInfo', 'id_01', 'id_31', 'id_14', 'DeviceType', 'id_32', 'TransactionID']


transDataTrain = pd.read_csv('./train_transaction.csv', usecols=transCol) 
transDataTest = pd.read_csv('./test_transaction.csv', usecols=transCol[1:])
idDataTest = pd.read_csv('./test_identity.csv', usecols=idCol)
idDataTrain = pd.read_csv('./train_identity.csv',usecols=idCol)
print ("trasaction data size: ", transDataTrain.shape, transDataTest.shape )
print ("identity data size: ", idDataTrain.shape, idDataTest.shape)

mergeData = pd.merge(transDataTrain,idDataTrain, how = 'left', on='TransactionID')
print ("Merged Train data size: ", mergeData.shape)
mergeData.head()

mergeDataTest = pd.merge(transDataTest,idDataTest, how = 'left', on='TransactionID')
print ("Merged Test data size: ", mergeDataTest.shape)
mergeDataTest.head()

def enable_plotly_in_cell():
  import IPython
  from plotly.offline import init_notebook_mode
  display(IPython.core.display.HTML('''<script src="/static/components/requirejs/require.js"></script>'''))
  init_notebook_mode(connected=False)

"""# Part 1 - Fraudulent vs Non-Fraudulent Transaction"""

def q1plotter(col, textTitle="Dist", numClasses=0, height=500, width=800, hspace=0.1, distPlot=False):
  enable_plotly_in_cell()
  fig = make_subplots(rows=1, cols=2, subplot_titles=['Non-Fradulent Transaction', 'Fraudulent Transaction'], horizontal_spacing=hspace)
  for i in range(2):
    if(numClasses==-1):
      numClasses=0
    if(numClasses>0):
      x = list(mergeData[mergeData.isFraud==i][col].value_counts().index)[:numClasses]
      y = np.sum(list(mergeData[mergeData.isFraud==i][col].value_counts())[numClasses:])
      data = list(mergeData[mergeData.isFraud==i][mergeData[col].isin(x)][col])
      data += ['Others']*y
      fig.append_trace(go.Histogram(x=data), row=1, col=i+1)
    else:
      fig.append_trace(go.Histogram(x=mergeData[mergeData.isFraud==i][col].values), row=1, col=i+1)
  fig.update_layout(height = height, width = width, title_text=textTitle)
  fig.update_layout(showlegend=False)
  fig.show()

q1plotter('DeviceType', "Distribution of Device Type", height=400, width = 600)

"""**From the distributions, we can infer that people prefer desktop over mobile. 
However given that a transaction is fraudulent, the probability of mobile and desktop are same. (Apriori probability)**
"""

q1plotter('DeviceInfo', textTitle='Distribution of Device Info', numClasses=9, width=1000)
# By setting the numClasses to -1, all the categories of Device Info can be seen in the distribution.

"""**From the distibutions, despite Windows and Apple being big companies, these distributions raise a question about their security. However, this could be wrong at arriving at such a hasty inference, because given that the majority of world's population are consumers of these two popular brands, the % population that are involved in fraudulant transaction is insignificant.**"""

q1plotter('TransactionAmt', textTitle='Distribution of TransactionAmt - Zoom the graph for a granular picture', width=1000)

"""**This is an interesting graph as I was expecting the fraudulant transactions to be concentrated on much higher monetary value. However, we can see that the density of distribution is mich higher for TransactionAmt $10-60 than the non-fraudulant transactions**"""

q1plotter('ProductCD', 'Distribution of ProductCD')

"""**This gives away the fact that the probability that the product Code is C given that the transaction is fraudulant is highest. Although W has a large number, it has a small ratio when compared against the total number of transactions it was a part of.**"""

q1plotter('card4', 'Distribution of card4')

"""**This plot tells that people prefer Visa over all the other card types. ~3.5% of VISA and mastercard, 8% of discover and ~3% of american express are fraudulant. So people prefer discover the least. This is justified. VISA and Mastercard probably have better plans offering for the customers.**"""

q1plotter('card6', 'Distribution of card6')

"""**Credit cards are more involved in more fraudulant transactions than the credit cards according to the ratio. This is doesn't make sense so much. Banks try to drive more sale of credit cards but they dont give enogh security for it.**"""

q1plotter('P_emaildomain', 'Distribution of P_emaildomain', numClasses=-1)

"""**This is similar to the distribution of Device Info. (Inferentially)**"""

q1plotter('R_emaildomain', 'Distribution of R_emaildomain', numClasses=8)

"""**Inference same as the above**"""

q1plotter('dist1', 'Distribution of dist1')

"""**These distributions make a lot of sense. A lot of transactions on the graph in the left suggest that more transactions happen inside a 4 mile radius. Whereas the first bucket in the right graph uptill is 19 miles - much plausible for a fraudulant transaction. There is two  points in the right graph at ~4900 miles. Which could be case where cards were stolen.**"""

q1plotter('dist2', 'Distribution of dist2')

"""**This looks very similar to the above distribution.**"""

q1plotter('addr1', 'Distribution of addr1')

"""**This should probably be the billing county and the spikes you see might be the location of shopping complex or a shopping area.**"""

q1plotter('addr2', 'Distribution of addr2')

"""**This should be the representative of the country. The majority data being collected from one country and the rest could be people touring/visiting other countries.**

# Part 2 - Transaction Frequency
"""

np.max(mergeData.TransactionDT), min(mergeData.TransactionDT)

86400//(60*60*12)

"""**The next two cells have the plots of distribution of number of transactions with respect to the hour of day**"""

enable_plotly_in_cell()
for i in list(mergeData.addr2.value_counts().index)[:2]:
  trace = go.Histogram(x = mergeData[mergeData.addr2==i].TransactionDT//(60*60)%24)
  data = [trace]
  layout = go.Layout(title=go.layout.Title(text='Distribution of Transactions over the hours of the day for addr2: '+str(int(i))),
                    xaxis=go.layout.XAxis(tickmode='linear', tick0=0, dtick=1,
                                          title=go.layout.xaxis.Title(text="Hours of the day (sequential but uncalibrated)")))
  fig = go.Figure(data=data, layout=layout)
  fig.update_layout(height = 300, width = 800)
  iplot(fig)

"""**These plots show that addr2: 60 and 87 almost belong to the same time zones. Represented by the activity of the people.**"""

enable_plotly_in_cell()
for i in list(mergeData.addr2.value_counts().index)[2:5]:
  trace = go.Histogram(x = mergeData[mergeData.addr2==i].TransactionDT//(60*60)%24)
  data = [trace]
  layout = go.Layout(title=go.layout.Title(text='Distribution of Transactions over the hours of the day for addr2: '+str(int(i))),
                    xaxis=go.layout.XAxis(tickmode='linear', tick0=0, dtick=1,
                                          title=go.layout.xaxis.Title(text="Hours of the day (sequential but uncalibrated)")))
  fig = go.Figure(data=data, layout=layout)
  fig.update_layout(height = 300, width = 800)
  iplot(fig)

"""**The following addr2s show some shift in the timezones/ lifestyles of the people. We can see from the bucketsizes that the it has been less used in these countries.**

# Part 3 - Product Code
"""

enable_plotly_in_cell()
data = []
x = [i for i in mergeData.ProductCD.unique()]
y = [np.sum(mergeData[mergeData.ProductCD==i].TransactionAmt.values) for i in x]
data.append(go.Bar(x=x, y=y))
layout = go.Layout(title=go.layout.Title(text='Distribution of TransactionAmt for ProductCD'), 
                   yaxis= go.layout.YAxis(title=go.layout.yaxis.Title(text='Total $')),
                   xaxis= go.layout.XAxis(title=go.layout.xaxis.Title(text='ProductCD')))
fig = go.Figure(data=data, layout=layout)
iplot(fig)

"""**W is the most frequently sold item. Should probably be daily comodities.*
 Can't guess much about the others**
"""

def q3Plotter(productCD, width=800, height=300):
  enable_plotly_in_cell()
  fig = make_subplots(rows=1, cols=2, subplot_titles=['Histogram', 'Violin Plot'], horizontal_spacing=0.1)
  fig.append_trace(go.Histogram(x = mergeData[mergeData.ProductCD==productCD].TransactionAmt), row=1, col=1)
  fig.append_trace(go.Violin(x = mergeData[mergeData.ProductCD==productCD].TransactionAmt,  box_visible=True), row=1, col=2)
  #trace = go.Histogram(x = mergeData[mergeData.ProductCD==i].TransactionAmt)
  #data = [trace]
  #layout = go.Layout(title=go.layout.Title(text='Distribution of TransactionAmt for ProductCD: '+i),
  #                  xaxis=go.layout.XAxis(title=go.layout.xaxis.Title(text="$")))
  #fig = go.Figure(data=data, layout=layout)
  fig.update_layout(height = height, width = width, showlegend=False, title_text="Distribution of TransactionAmt for ProductCD: "+productCD)
  iplot(fig)

mergeData.ProductCD.unique()

"""**Drag the cursor to zoom, double click to resize**"""

q3Plotter('W')

q3Plotter('H')

q3Plotter('C')

q3Plotter('S')

q3Plotter('R')

"""**From the violin plots, the outliers were observed and the estimate of the min, max, 1st quartile, median and 3rd quartile were extracted and tabulated.**"""

productCDBounds = {'W':{ 
                          'min': 1,
                          'lower': 1, 
                          'quad1': 49,
                          'median': 78.5,
                          'quad3': 146, 
                          'upper': 291.27,
                          'max': 31937.39
                        },
                    'H':{
                          'min': 15,
                          'lower': 15,
                          'quad1': 35,
                          'median': 50,
                          'quad3': 100,
                          'upper': 190,
                          'max': 500 
                        },
                    'C':{
                          'min': 0.25,
                          'lower': 0.25,
                          'quad1': 18.42,
                          'median': 31.19,
                          'quad3': 54.10,
                          'upper': 107.59,
                          'max': 712.89
                        },
                    'S':{
                          'min': 5,
                          'lower': 5,
                          'quad1': 20,
                          'median': 35,
                          'quad3': 80,
                          'upper': 170,
                          'max': 1550 
                        },
                    'R':{
                          'min': 25,
                          'lower': 25,
                          'quad1': 100,
                          'median': 125,
                          'quad3': 200,
                          'upper': 350,
                          'max': 1800
                        }
                   }

prices = pd.DataFrame(productCDBounds).T
prices = prices[['min', 'lower', 'quad1', 'median', 'quad3', 'upper', 'max']]
prices

"""**From the table above, it very clear that R is the most expensive product by comparing the quartile values median and the upper bound.**

# Part 4 - Correlation Coefficient
"""

corrData = mergeData[['TransactionAmt', 'TransactionDT']]
corrData['TransactionDT'] = (corrData['TransactionDT']//(60*60))%24
print ("Pearson: \n",corrData.corr(method='pearson'))
print ("Spearman: \n",corrData.corr(method='spearman'))

"""**The correlations are not evident as one hour is a long duration. The range of sale values will be very big, hence the correlation value would not be very useful**"""

for i in mergeData.ProductCD.unique():
  tempData = mergeData[mergeData.ProductCD == i][['TransactionAmt', 'TransactionDT']]
  tempData = tempData[(tempData.TransactionAmt >= productCDBounds[i]['lower'])&(tempData.TransactionAmt <= productCDBounds[i]['upper'])]
  print ('ProductCD: '+i)
  print ("Pearson: \n",tempData.corr(method='pearson'))
  print ("Spearman: \n",tempData.corr(method='spearman'))
  print ('----------')

"""**Pearson and spearman correlation of each of he productCD between TransactionAmt and TransactionDT. The smaller values close to 0 still tel us that the range problem exists. However, for  product S there is a small positive correlation.**

# Q5 - Interesting plots
"""

def q4Plotter(productCD = 'W', addr2 = 87):
  enable_plotly_in_cell()
  data = []
  for i in range(0,24):
    cond1 = (mergeData.ProductCD==productCD)
    p = ((mergeData.TransactionDT//(60*60))%24 == i)
    data.append(go.Violin(y = mergeData[p][cond1&(mergeData.addr2==addr2)&(mergeData.TransactionAmt>=productCDBounds[productCD]['lower'])&
                                        (mergeData.TransactionAmt<=productCDBounds[productCD]['upper'])].TransactionAmt, 
                          x = (mergeData[p][cond1&(mergeData.addr2==addr2)&(mergeData.TransactionAmt>=productCDBounds[productCD]['lower'])&
                                        (mergeData.TransactionAmt<=productCDBounds[productCD]['upper'])].TransactionDT//(60*60))%24, 
                          name = i, box_visible=True, meanline_visible=True)
    )
  
  fig = go.Figure(data=data)
  fig.update_layout(title_text="Violin Plot distribution of TransactionAmt over the hours of the day for ProductCD: "+ productCD,
                    xaxis=go.layout.XAxis(tickmode='linear', tick0=0, dtick=1,
                                          title=go.layout.xaxis.Title(text="Hours of the day (sequential but uncalibrated)")),
                    yaxis=go.layout.YAxis(title=go.layout.yaxis.Title(text="$")))
  fig.update_layout(showlegend=False)
  iplot(fig)

q4Plotter(productCD='W')

q4Plotter(productCD='C')

q4Plotter(productCD='R')

q4Plotter(productCD='H')

q4Plotter(productCD='S')

"""**From the graphs above we can see that**


*   Product W sales is very uniform overall but, there are subtle differences in the density of distribution across the hour of the day.
*   Product C shows a clearly visible trend wherein the product isn't bought during most hours of the day. It could probably be something like a furniture or jewellery.
*   The other products have differences in distribution of sale value through the day.
*   These graphs were plotted only for addr2=87 as it constitutes the majority and belongs to one timezone.

# Part 6 - Prediction Model
"""

mergeData.columns

mergeData['hour'] = (mergeData.TransactionDT//(60*60))%24
mergeData['day'] = (mergeData.TransactionDT//(60*60*24))%7

mergeDataTest['hour'] = (mergeData.TransactionDT//(60*60))%24
mergeDataTest['day'] = (mergeData.TransactionDT//(60*60*24))%7

mergeData.columns, mergeDataTest.columns

cols = ['isFraud', 'C1', 'card1', 'TransactionAmt', 'card2', 'C7', 'C13', 'C14', 'P_emaildomain',  'D15', 'card5', 'C2', 'dist1',
        'M1', 'ProductCD', 'card6', 'card4', 'id_02', 'DeviceInfo', 'id_01', 'id_31', 'id_14', 'DeviceType', 'id_32']

modelData = mergeData[cols[:16]]
modelDataTest = mergeDataTest[cols[1:16]]

modelData.head()

from sklearn.model_selection import train_test_split, cross_val_predict
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
import lightgbm as lgb
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from imblearn.over_sampling import SMOTE, ADASYN

y = modelData.isFraud
modelData.drop(columns=['isFraud'], inplace=True)

totalData = pd.concat([modelData, modelDataTest], axis=0)

for i in totalData.columns:
  print (i, np.sum(totalData[i].isna()), totalData[i].dtype)

sorted(totalData.P_emaildomain.unique()[1:])

totalData.P_emaildomain.fillna(value='_', inplace=True)
for i in totalData['P_emaildomain'].unique():
  if(i == np.nan):
    continue
  totalData.replace({i:i.split('.')[0]},inplace=True)

totalData['P_emaildomain'].unique()

labels, _ = pd.factorize(totalData['P_emaildomain'])
totalData.drop(columns=['P_emaildomain'], inplace=True)
totalData['P_emaildomain'] = labels

labels, _ = pd.factorize(totalData['ProductCD'])
totalData.drop(columns=['ProductCD'], inplace=True)
totalData['ProductCD'] = labels

labels, _ = pd.factorize(totalData['M1'])
totalData.drop(columns=['M1'], inplace=True)
totalData['M1'] = labels

labels, _ = pd.factorize(totalData['card6'])
totalData.drop(columns=['card6'], inplace=True)
totalData['card6'] = labels

for i in totalData.columns:
  print(i,np.sum(pd.isna(totalData[i])))
  if(np.sum(pd.isna(totalData[i]))):
    totalData[i].fillna(value=-1, inplace=True)

totalData.head()

totalData.shape[0] - modelDataTest.shape[0] == modelData.shape[0]

modelData = totalData.iloc[:modelData.shape[0],:]
modelData['isFraud'] = y

modelDataTest = totalData.iloc[modelData.shape[0]:]

modelData.head()

X_train, X_test, Y_train, Y_test = train_test_split(modelData.values[:,:-1], modelData.values[:,-1] ,test_size=0.2, random_state=101 )

print (X_train.shape, X_test.shape)
print (Y_train.shape, Y_test.shape)

Counter(Y_train), Counter(Y_test)

for i in totalData.columns:
  print (i, np.sum(totalData[i].isna()), totalData[i].dtype)

lr = LogisticRegression()
lr.fit(X_train, Y_train)
Y_pred = lr.predict(X_test)
print (classification_report(Y_test, Y_pred))
print (roc_auc_score(Y_test, Y_pred))

dt = DecisionTreeClassifier()
dt.fit(X_train, Y_train)
Y_pred = dt.predict(X_test)
targetNames = ['0', '1']
print (classification_report(Y_test, Y_pred, target_names=targetNames))
print (roc_auc_score(Y_test, Y_pred))

et = ExtraTreesClassifier(n_estimators=200, n_jobs=-1)
et.fit(X_train, Y_train)
Y_pred = et.predict(X_test)
print (classification_report(Y_test, Y_pred, target_names=targetNames))
print (roc_auc_score(Y_test, Y_pred))

params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
#    'metric': {'l2', 'l1'},
    'learning_rate': 0.005,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
 #   'bagging_freq': 5,
    'verbose': 2,
    'metric': 'binary_logloss'
}
lgb_train = lgb.Dataset(X_train, Y_train)
lgb_eval = lgb.Dataset(X_test, Y_test)
lgbm = lgb.train(params=params, train_set= lgb_train, num_boost_round=10000)

Y_pred = lgbm.predict(X_test, num_iteration=lgbm.best_iteration)
plt.hist(Y_test[Y_test==1] - Y_pred[Y_test==1])
plt.show()

"""# Part 7 - Final Result

## Since we are dealing with class imbalance problem, we can counter it by oversampling techniques. I didn't go for undersampling as we are already in dearth of data
"""

X_resampled, y_resampled = SMOTE().fit_resample(X_train, Y_train)
print(sorted(Counter(y_resampled).items()))
clf_smote = ExtraTreesClassifier(n_estimators=100, n_jobs=-1).fit(X_resampled, y_resampled)
Y_pred = clf_smote.predict(X_test)
print (classification_report(Y_test, Y_pred))
print (roc_auc_score(Y_test, Y_pred))

X_resampled, y_resampled = SMOTE(n_jobs=-1).fit_resample(modelData.values[:,:-1], modelData.values[:,-1])
print(sorted(Counter(y_resampled).items()))
X_tr, X_te, Y_tr, Y_te = train_test_split(X_resampled, y_resampled)
clf_smote = ExtraTreesClassifier(n_estimators=50, n_jobs=-1).fit(X_tr, Y_tr)
Y_pred = clf_smote.predict(X_te)
print (classification_report(Y_te, Y_pred))
print (roc_auc_score(Y_te, Y_pred))

clf_smote = ExtraTreesClassifier(n_estimators=10, n_jobs=-1).fit(X_tr, Y_tr)
Y_pred = clf_smote.predict(X_te)
print (classification_report(Y_te, Y_pred))
print (roc_auc_score(Y_te, Y_pred))

"""**This is the final model. I felt I was overfitting it a bit,but when I reduced the number of estimators, it showed no change. The result it produced in kaggle with roc_auc of 0.72**"""

Y_pred = clf_smote.predict(modelDataTest.values)
np.sum(Y_pred)/len(Y_pred)

submissionFile = np.vstack((mergeDataTest.TransactionID.values, np.array(Y_pred))).T
submissionFile = pd.DataFrame(submissionFile, columns=['TransactionID', 'isFraud'])

submissionFile.astype(int).to_csv('Submission.csv', index=False)

"""1.   Snapshot of leaderboard: [Here](https://drive.google.com/open?id=1HBNEgTG81RdgdDt3ORVf1R5WHdNQce8V) Only viewable from stony brook CS email id
2.   Kaggle link: [Here](https://www.kaggle.com/adithya8/account) This is on my personal email id as I thought I was registered on my stonybrook email id not until the final day when the registration closed. However I have added my affiliation to be Stony Brook university
3.   Highest Rank: 5508
4.   Score: (roc_auc) 0.7241
5.   Number of entries: 2
6.   Link to submission file: [Here](https://drive.google.com/open?id=1AG69ke8A6seyBwhyxFzHcnZYAa-bAt8h) Viewable only from Stony Brook University CS email id

---

# Few Graphs for understanding the distributions and auto-correlative properties
"""

plt.figure(figsize=(10,10))
plt.grid(True)
plt.hist(mergeData.TransactionDT//(60*60*12), bins=50)
plt.show()

plt.figure(figsize=(10,10))
plt.grid(True)
plt.hist(mergeData[mergeData.isFraud==1].TransactionDT//(60*60*12), bins=181)
plt.show()

mergeData['TransTransactionDT'] = mergeData.TransactionDT//(60*60*12)

metaData = []
metaCol = ['semiDay', 'Transactions', 'Fradulant', 'TransactionAmt', 'FraudAmt']
for i in range(np.min(mergeData.TransTransactionDT),np.max(mergeData.TransTransactionDT)+1):
  temp = []
  temp.append(i)
  temp.append(mergeData[mergeData.TransTransactionDT==i].TransactionID.unique().shape[0])
  temp.append(mergeData[(mergeData.TransTransactionDT==i)&(mergeData.isFraud==1)].TransactionID.shape[0])
  temp.append(np.sum(mergeData[mergeData.TransTransactionDT==i].TransactionAmt))
  temp.append(np.sum(mergeData[(mergeData.TransTransactionDT==i)&(mergeData.isFraud==1)].TransactionAmt))
  if (i%100==0):
    print (temp)
  metaData.append(temp)

metaData = pd.DataFrame(metaData, columns=metaCol)
print ("Meta size: ", metaData.shape)
metaData.head()

plt.figure(figsize=(16,10))
plt.grid(True)
plt.bar(metaData.semiDay, metaData.Transactions, align='center', alpha = 0.9)
plt.show()

plt.figure(figsize=(16,10))
plt.grid(True)
plt.scatter(metaData.semiDay, metaData.TransactionAmt)
plt.show()

plt.figure(figsize=(16,10))
plt.grid(True)
plt.bar(metaData.semiDay, metaData.Fradulant, align='center', alpha = 0.9)
plt.show()

plt.figure(figsize=(16,10))
plt.grid(True)
plt.scatter(metaData.semiDay, metaData.FraudAmt)
plt.show()

plot_acf(metaData.Transactions, lags=20 )
plt.show()

plt.figure(figsize=(12,10))
plt.acorr(metaData.TransactionAmt, maxlags=15)
plt.show()