# -*- coding: utf-8 -*-
"""HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r_JPRod2kRb8BXbDawWwgYXTNLMroLAU

# HW 3

## Imports and data loading
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd 
import numpy as np
import plotly.graph_objects as go
import matplotlib.pyplot as plt
from matplotlib import rcParams
import seaborn as sns
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from plotly.offline import init_notebook_mode, iplot
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
from collections import Counter, OrderedDict
from pprint import pprint
# %matplotlib inline 

from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import NearestNeighbors
from sklearn.model_selection import permutation_test_score
from sklearn import preprocessing
from sklearn import cluster
from sklearn import metrics
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.linear_model import Lasso
from sklearn.ensemble import GradientBoostingRegressor

from google.colab import drive
drive.mount('/content/drive')

cd drive/My\ Drive/CSE 519/HW3

# Commented out IPython magic to ensure Python compatibility.
# %ls

data = pd.read_csv('./train.csv', low_memory=False)

data.shape

desc = '''
SalePrice : the property's sale price in dollars. This is the target variable that you're trying to predict.
MSSubClass: The building class
MSZoning: The general zoning classification
LotFrontage: Linear feet of street connected to property
LotArea: Lot size in square feet
Street: Type of road access
Alley: Type of alley access
LotShape: General shape of property
LandContour: Flatness of the property
Utilities: Type of utilities available
LotConfig: Lot configuration
LandSlope: Slope of property
Neighborhood: Physical locations within Ames city limits
Condition1: Proximity to main road or railroad
Condition2: Proximity to main road or railroad (if a second is present)
BldgType: Type of dwelling
HouseStyle: Style of dwelling
OverallQual: Overall material and finish quality
OverallCond: Overall condition rating
YearBuilt: Original construction date
YearRemodAdd: Remodel date
RoofStyle: Type of roof
RoofMatl: Roof material
Exterior1st: Exterior covering on house
Exterior2nd: Exterior covering on house (if more than one material)
MasVnrType: Masonry veneer type
MasVnrArea: Masonry veneer area in square feet
ExterQual: Exterior material quality
ExterCond: Present condition of the material on the exterior
Foundation: Type of foundation
BsmtQual: Height of the basement
BsmtCond: General condition of the basement
BsmtExposure: Walkout or garden level basement walls
BsmtFinType1: Quality of basement finished area
BsmtFinSF1: Type 1 finished square feet
BsmtFinType2: Quality of second finished area (if present)
BsmtFinSF2: Type 2 finished square feet
BsmtUnfSF: Unfinished square feet of basement area
TotalBsmtSF: Total square feet of basement area
Heating: Type of heating
HeatingQC: Heating quality and condition
CentralAir: Central air conditioning
Electrical: Electrical system
1stFlrSF: First Floor square feet
2ndFlrSF: Second floor square feet
LowQualFinSF: Low quality finished square feet (all floors)
GrLivArea: Above grade (ground) living area square feet
BsmtFullBath: Basement full bathrooms
BsmtHalfBath: Basement half bathrooms
FullBath: Full bathrooms above grade
HalfBath: Half baths above grade
BedroomAbvGr: Number of bedrooms above basement level
KitchenAbvGr: Number of kitchens
KitchenQual: Kitchen quality
TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)
Functional: Home functionality rating
Fireplaces: Number of fireplaces
FireplaceQu: Fireplace quality
GarageType: Garage location
GarageYrBlt: Year garage was built
GarageFinish: Interior finish of the garage
GarageCars: Size of garage in car capacity
GarageArea: Size of garage in square feet
GarageQual: Garage quality
GarageCond: Garage condition
PavedDrive: Paved driveway
WoodDeckSF: Wood deck area in square feet
OpenPorchSF: Open porch area in square feet
EnclosedPorch: Enclosed porch area in square feet
3SsnPorch: Three season porch area in square feet
ScreenPorch: Screen porch area in square feet
PoolArea: Pool area in square feet
PoolQC: Pool quality
Fence: Fence quality
MiscFeature: Miscellaneous feature not covered in other categories
MiscVal: $Value of miscellaneous feature
MoSold: Month Sold
YrSold: Year Sold
SaleType: Type of sale
SaleCondition: Condition of sale
'''

descDict = OrderedDict()
desc = desc.split('\n')
desc = [i.split(':') for i in desc]
desc = desc[1:-1]
for i in desc:
  descDict[i[0].strip()] = i[1].strip()
descDict

for i, j in enumerate(descDict):
  print(i, j, ':', descDict[j])

"""## Part 1 - Pairwise Correlations

- Approach: Splitting the data into 4 sets and plotting the individual correlations with SallePrice.
- Handpicking the interesting ones and then plotting the final correlation plot.
"""

decCol = []
objCol = []
p = 0
for i in data.columns:
  if('int' in str(data[i].dtype) or 'float' in str(data[i].dtype)):
    print (p, i, data[i].dtype)
    decCol.append(i)
    p+=1 
objCol = list(set(data.columns) - set(decCol))
decCol = decCol[:-1]

rcParams['figure.figsize'] = 14,10
sns.heatmap(data[decCol[:15]+['SalePrice']].corr(), annot=True )

sns.heatmap(data[decCol[14:28]+['SalePrice']].corr(), annot=True )

"""**People generally like to have fewer kitchens, hence the negative correlation**"""

sns.heatmap(data[decCol[28:]+['SalePrice']].corr(), annot=True )

for i in objCol:
  print (i, (Counter(data[i])))

"""1. **Roughly well distributed**: CentralAir, Neighborhood, Exterior2nd, KitchenQual, HeatingQC, GarageFinish, ExterQual, LotShape, Foundation, BsmtQual, BsmtFinType1, Exterior1st
2. **Skewed**: FireplaceQu, MasVnrType, BsmtExposure, GarageType, BldgType, Fence, HouseStyle
"""

objColStr = ['HouseStyle', 'Condition1', 'Condition2' ,'CentralAir', 'Neighborhood', 'Exterior2nd', 'KitchenQual', 'GarageQual', 'HeatingQC', 'GarageFinish', 'ExterQual', 'LotShape', 'FireplaceQu', 'Foundation', 'BsmtQual', 'BsmtFinType1', 'Exterior1st', 'MasVnrType', 'BsmtExposure', 'GarageType', 'BldgType', 'Fence']

print ('COLUMN NAME - % NULL VALUES')
for i in objColStr:
  print (i, np.sum(pd.isna(data[i]))*100.0/len(data))

"""- APPROACH: While encoding categorical features, some categories revealed a different distributions depending upon the categorical value they took. 
- Hence those columns were sorted with respect to their mean and std deviation, and were encoded from 1 to N.
- This reflected in the correlation chart with high correlation with Saleprice.
"""

sortedIndexN = pd.concat([data.groupby('Neighborhood').SalePrice.mean(), data.groupby('Neighborhood').SalePrice.std()], axis=1)
sortedIndexN.columns = list(['meanVal', 'stdVal'])
sortedIndexN = sortedIndexN.sort_values(['stdVal'])
sortedIndexN = sortedIndexN.sort_values(['meanVal'], ascending=False)
sortedIndexN.reset_index(inplace=True)
sortedIndexN

sortedIndexH = pd.concat([data.groupby('HouseStyle').SalePrice.mean(), data.groupby('HouseStyle').SalePrice.std()], axis=1)
sortedIndexH.columns = list(['meanVal', 'stdVal'])
sortedIndexH = sortedIndexH.sort_values(['stdVal'])
sortedIndexH = sortedIndexH.sort_values(['meanVal'], ascending=False)
sortedIndexH.reset_index(inplace=True)
sortedIndexH

sortedIndexL = pd.concat([data.groupby('LotShape').SalePrice.mean(), data.groupby('LotShape').SalePrice.std()], axis=1)
sortedIndexL.columns = list(['meanVal', 'stdVal'])
sortedIndexL = sortedIndexL.sort_values(['stdVal'])
sortedIndexL = sortedIndexL.sort_values(['meanVal'], ascending=False)
sortedIndexL.reset_index(inplace=True)
sortedIndexL

dataObj = data[objColStr]
dataObj['Neighborhood'] = dataObj['Neighborhood']. replace(dict(zip(sortedIndexN.Neighborhood, reversed(range(1,len(sortedIndexN)+1)))))
dataObj['HouseStyle'] = dataObj['HouseStyle']. replace(dict(zip(sortedIndexH.HouseStyle, reversed(range(1,len(sortedIndexH)+1)))))
dataObj['LotShape'] = dataObj['LotShape']. replace(dict(zip(sortedIndexL.LotShape, reversed(range(1,len(sortedIndexL)+1)))))
objColStr.remove('Neighborhood')
objColStr.remove('HouseStyle')
objColStr.remove('LotShape')
for i in objColStr:
  dataObj[i].fillna(0, inplace=True)
  sortedIndex = dataObj[i].value_counts().index
  if ('Qu' in i or 'QC' in i):
    dataObj[i].replace({'Po':1 ,'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)
  else:
    dataObj[i] = dataObj[i].map(dict(zip(sortedIndex, range(1, len(sortedIndex)+1))))
dataObj['SalePrice'] = data['SalePrice']

rcParams['figure.figsize'] = 16,11
sns.heatmap(dataObj.corr(), annot=True)

interestingCol = ['Neighborhood', 'HouseStyle', 'GarageFinish', 'LotShape', 'GarageQual', 'BsmtQual', 'FireplaceQu', 'MasVnrType', 'KitchenQual', 'ExterQual', 'WoodDeckSF', 'OpenPorchSF', 'Fireplaces', 'TotRmsAbvGrd', 'GarageArea', 'GarageCars', 'GarageYrBlt', 'FullBath', 'GrLivArea', 'TotalBsmtSF', '1stFlrSF', 'MasVnrArea', 'YearBuilt', 'YearRemodAdd', 'OverallQual', 'BedroomAbvGr', 'KitchenAbvGr']

df = pd.concat([dataObj[interestingCol[:7]], data[interestingCol[7:]], data['SalePrice']], axis=1)
sns.heatmap(df.corr(), annot=True)

"""**Inference**
- Our approach of encoding the Categorical columns: Neighborhood, LotShape and HouseStyle resulted in good correlation values. 
- The OverallQual and qualities, and the fields with Area is highly correlated with the SalePrice, which doesn't surprise us.
- The fireplace is highly correlated with Saleprice, which talks about the importance of Fireplace in a location like IOWA.
- The number of Kitchens is negatively correlated with the price. It makes sense because people mostly like to have no more than 1 kitchen.
- There are trivial high correaltions between garage area and number, Total Rooms and LivArea of cars that also show up.
- Not really able to explain well GarageQual and GarageFinish are inveresely correlated because a different encoding was performed.

## Part 2 - Informative Plots
"""

for i in df.columns:
  print (i,':', descDict[i], ':', data[i].dtype)

rcParams['figure.figsize'] = 16,8
g = sns.boxenplot(x=dataObj['Neighborhood'], y=data['SalePrice'])
g.set_xticklabels(reversed(list(sortedIndexN.Neighborhood)), rotation=90)
g.set_title("Box Plots of Sale Price for each Neighborhood")

"""**Inference: This clearly shows what our embeddings are based on. Higher mean value gets higher encoding. There is very high amount of information in this embedding towards SalePrice**"""

rcParams['figure.figsize'] = 16,8
g = sns.countplot(dataObj['Neighborhood'])
g.set_ylabel("Number of houses")
g.set_xticklabels(reversed(list(sortedIndexN.Neighborhood)), rotation=90)
g.set_title("Box Plots of number of houses for each Neighborhood")

"""- **Inference: This plot and the plot above together mean something different. The plot here says where most people prefer to live. From this plot we see that NAmes has a lower sale price and a lot of houses. The less expensive ones are fewer in number and so are the more expensive ones as well. Kind of Normal in behavior.**

- **The CollegeCr has a lot of houses, which talks about the student population in that region.**
"""

rcParams['figure.figsize'] = 16,8
g = sns.boxenplot(x=dataObj['Neighborhood'], y=data['SalePrice']/data['LotArea'])
g.set_ylabel("Cost per SF")
g.set_xticklabels(reversed(list(sortedIndexN.Neighborhood)), rotation=90)
g.set_title("Box plot of Cost per SF (LotArea) across Neighborhoods")

"""**Inference: This explains why places like Blueste, MeadowV, BrDaleBlmgtn have fewer houses to sell. They have very high cost per SF. This is a question of affordability. Note that CollegeCr is much affordable for the college students.**"""

rcParams['figure.figsize'] = 16,8
g = sns.boxenplot(x=dataObj['Neighborhood'], y=np.sqrt(data['LotArea']))
g.set_xlabel("Sqrt of Lot Area")
g.set_xticklabels(reversed(list(sortedIndexN.Neighborhood)), rotation=90)
g.set_title("Box Plots of Lot Area for each Neighborhood")

"""**Inference: This was plotted to do a check by multiplexing the information from the two graphs above. To make sure if they make sense. The expensive plots are smaller in sizes.**"""

plt.figure(figsize=(10,5))
plt.grid(True)
plt.scatter((data['OverallQual']), data['SalePrice'])
plt.plot(data.groupby('OverallQual').groups.keys(), data.groupby('OverallQual').SalePrice.mean(), label='mean SalePrice')
plt.legend()
plt.ylabel('SalePrice USD')
plt.xlabel('OverallQual')
plt.title("SalePrice vs OverallQual")
plt.show()

plt.figure(figsize=(10,5))
plt.grid(True)
plt.scatter(dataObj['ExterQual'], data['SalePrice'])
p = data.groupby('ExterQual').SalePrice.mean()
p.index = p.index.map({'Po':1 ,'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},na_action=0)
p.sort_index(inplace=True)
plt.plot(p.index, p.values, label='mean SalePrice')
plt.legend()
plt.ylabel('SalePrice USD')
plt.xlabel('ExterQual')
plt.title("SalePrice vs ExterQual")
plt.show()

plt.figure(figsize=(10,5))
plt.grid(True)
plt.scatter(dataObj['KitchenQual'], data['SalePrice'])
p = data.groupby('KitchenQual').SalePrice.mean()
p.index = p.index.map({'Po':1 ,'Fa':2, 'TA':3, 'Gd':4, 'Ex':5},na_action=0)
p.sort_index(inplace=True)
plt.plot(p.index, p.values, label='mean SalePrice')
plt.legend()
plt.ylabel('SalePrice USD')
plt.xlabel('KitchenQual')
plt.title("SalePrice vs KitchenQual")
plt.show()

plt.figure(figsize=(10,5))
plt.grid(True)
plt.scatter(dataObj['BsmtQual'], data['SalePrice'])
p = data.fillna(0).groupby('BsmtQual').SalePrice.mean()
p.index = p.index.map({0:0 ,'Po':1 ,'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})
p.sort_index(inplace=True)
plt.plot(p.index, p.values, label='mean SalePrice')
plt.legend()
plt.ylabel('SalePrice USD')
plt.xlabel('BsmtQual')
plt.title("SalePrice vs BsmtQual")
plt.show()

plt.figure(figsize=(10,5))
plt.grid(True)
plt.scatter(dataObj['GarageQual'], data['SalePrice'])
p = data.fillna(0).groupby('GarageQual').SalePrice.mean()
p.index = p.index.map({0:0, 'Po':1 ,'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})
p.sort_index(inplace=True)
plt.plot(p.index, p.values, label='mean')
plt.legend()
plt.ylabel('SalePrice USD')
plt.xlabel('GarageQual')
plt.title("SalePrice vs GarageQual")
plt.show()

"""1. **Inference: We are able to notice that the least cost as well as the highest cost (upper bound) increases as the Quality increases. The increase is somewhat non-linear. NOTE that we shouldn't factor the outliers (Outliers can be seen at Overall Quality = 4,9,10).**

2. **The garage Qual has very little information to provide as the SalePrice value doesnt increase sharply as the GarageQual increases. A simple bias term can be used to rectify this**

3. **ExterQual and BsmtQual are les affected by outliers at rating=5 but KitchenQual seems to have a lot of outliers at 5**

4. **Lets take a look at the outliers below**
"""

data.sort_values(['SalePrice'])[data.OverallQual==10][list(df.columns) + ['Neighborhood']]

"""The Neighborhood like Edwards has a less Cost Per SF which was the reason why the Cost was less although the OverallQual was quite high. Wouldn't be surprised to find a couple of good quality houses in the least preferred neighborhoods."""

data.LotShape.unique()

fig, ax = plt.subplots(nrows = 2,ncols=2)
fig.set_tight_layout('pad')
ax[0, 0].hist(data[data.LotShape == 'Reg']['SalePrice'], bins=50)
ax[0, 0].set_xlabel("SalePrice")
ax[0, 0].set_ylabel("# houses")
ax[0, 0].set_title("Distribution of SalePrice for LotShape: Reg")
ax[0, 1].hist(data[data.LotShape == 'IR1']['SalePrice'], bins=50)
ax[0, 1].set_xlabel("SalePrice")
ax[0, 1].set_ylabel("# houses")
ax[0, 1].set_title("Distribution of SalePrice for LotShape: IR1")
ax[1, 0].hist(data[data.LotShape == 'IR2']['SalePrice'], bins=30)
ax[1, 0].set_xlabel("SalePrice")
ax[1, 0].set_ylabel("# houses")
ax[1, 0].set_title("Distribution of SalePrice for LotShape: IR2")
ax[1, 1].hist(data[data.LotShape == 'IR3']['SalePrice'], bins=20)
ax[1, 1].set_xlabel("SalePrice")
ax[1, 1].set_ylabel("# houses")
ax[1, 1].set_title("Distribution of SalePrice for LotShape: IR3")
plt.show()

"""**Inference: There is a change in distribution of SalePrice depending upon the LotShape.**"""

data['Foundation'].value_counts()

fig, ax = plt.subplots(nrows = 2,ncols=2, figsize=(10,10))
fig.set_tight_layout('pad')
ax[0, 0].hist(data[data.Foundation == 'PConc']['SalePrice'], bins=70)
ax[0, 0].set_xlabel("USD SalePrice")
ax[0, 0].set_ylabel("# houses")
ax[0, 0].set_title("Foundation: PConc")
ax[0, 1].hist(data[data.Foundation == 'CBlock']['SalePrice'], bins=70)
ax[0, 1].set_xlabel("USD SalePrice")
ax[0, 0].set_ylabel("# houses")
ax[0, 1].set_title("Foundation: CBlock")
ax[1, 0].hist(data[data.Foundation == 'BrkTil']['SalePrice'], bins=70)
ax[1, 0].set_xlabel("USD SalePrice")
ax[0, 0].set_ylabel("# houses")
ax[1, 0].set_title("Foundation: BrkTil")
ax[1, 1].hist(data[data.Foundation.isin(['Wood','Slab','Stone'])]['SalePrice'], bins=50)
ax[1, 1].set_xlabel("USD SalePrice")
ax[0, 0].set_ylabel("# houses")
ax[1, 1].set_title("Foundation: Wood/Slab/Stone")
plt.show()

"""**Inference: Similar to the above plot, we find the distribution of the SalePrice changes with different categories that Foundation can take.**

**This pattern was also checked for Housestyle and BldgType below**
"""

for i in data['HouseStyle'].unique():
  plt.figure(figsize=(10,5))
  plt.title(i)
  plt.hist(data[data.HouseStyle==i]['SalePrice'], bins=20)
  plt.ylabel("# houses")
  plt.xlabel("SalePrice")
  plt.show()

for i in data['BldgType'].unique():
  plt.figure(figsize=(10,5))
  plt.title(i)
  plt.hist(data[data.BldgType==i]['SalePrice'], bins=40)
  plt.show()

"""## Part 3 - Handcrafted Scoring Function"""

interestingCol + ['SalePrice']

"""**Intitution: Scoring val and**
1.   Neighborhood value directly proportional
2.   YearBuilt directly proportional
3.   YearRemodAdd directly Proportional
4.   GrLiveArea directly Proportional
5.   BedroomAbvGr directly proportional (more like parabolic/ modulo. Lets reiterate)
6.   KitchenAbvGr indirectly proportional
7.   TotalBsmtSF indirectly proportional
8.   SalePrice indirectly proportional
"""

scoringCols = ['Neighborhood', 'YearBuilt', 'YearRemodAdd', 'GrLivArea', 'BedroomAbvGr', 'KitchenAbvGr', 'TotalBsmtSF', 'SalePrice']

scoreDf = pd.concat([dataObj[['Neighborhood']], data[scoringCols[1:]]], axis=1)

scoreDf.head()

"""My understanding behind building the scoring function: 

1. If the sale price is less for a given setup (bedroom, kitchen, neighborhood etc) then it gets a good score)
2. Similarly lower the number of kitchens the better it is
3. All the other values would be preferred to be high: #bedrooms, YearBuilt, YearRemodAdd, Area, Neighborhood
"""

scoreDf.sort_values(by=['KitchenAbvGr'], ascending=False, inplace=True)
scoreDf.sort_values(by=['SalePrice', 'Neighborhood', 'BedroomAbvGr', 'GrLivArea', 'TotalBsmtSF', 'YearBuilt', 'YearRemodAdd'], ascending=True, inplace=True)

"""1. Skiena had cited in one of the posts in piazza that more expensive, large homes are the most desired. 
2. Hence I take the above features and I sort them in the order of precedence that I feel they are important. 
3. I sort the kitchen in the reverse order, because I believe that people prefer to have only one kitchen. Hence more kitchen should probably mean less desirable opposed to the ones have more kitchens in a similar setup. 
4. **NEIGHBORHOOD definitely matters. It is more desired to have a large apartment at Manhattan against a large house at Stony Brook. But I am not very sure if my model will be able to pick it up. *LETS SEE*.**
"""

'''brkt = set(scoreDf.BedroomAbvGr.astype('str') + scoreDf.KitchenAbvGr.astype('str'))
print ('br\tkt\t #')
for i in sorted(brkt):
  br, kt = int(i[0]), int(i[1])
  print (br,'\t', kt,'\t', scoreDf[(scoreDf.BedroomAbvGr==br) & (scoreDf.KitchenAbvGr==kt)].shape[0])
scoreDf = scoreDf[(scoreDf.BedroomAbvGr>0)&(scoreDf.KitchenAbvGr>0)]
scoreDf['score'] = 0.0
scoreDf.loc[scoreDf.BedroomAbvGr==1,'score'] = (np.array(range(1,len(scoreDf[scoreDf.BedroomAbvGr==1])+1))*100.0)/len(scoreDf[scoreDf.BedroomAbvGr==1])
scoreDf.loc[scoreDf.BedroomAbvGr==2,'score'] = (np.array(range(1,len(scoreDf[scoreDf.BedroomAbvGr==2])+1))*100.0)/len(scoreDf[scoreDf.BedroomAbvGr==2])
scoreDf.loc[scoreDf.BedroomAbvGr==3,'score'] = (np.array(range(1,len(scoreDf[scoreDf.BedroomAbvGr==3])+1))*100.0)/len(scoreDf[scoreDf.BedroomAbvGr==3])
scoreDf.loc[scoreDf.BedroomAbvGr>=4,'score'] = (np.array(range(1,len(scoreDf[scoreDf.BedroomAbvGr>=4])+1))*100.0)/len(scoreDf[scoreDf.BedroomAbvGr>=4])
scoreDf.reset_index(drop=True, inplace=True)
scoreDf.head()'''

scoreDf['score'] = (np.array(range(1,len(scoreDf)+1))*100.0)/len(scoreDf)
scoreDf.reset_index(drop=True, inplace=True)
scoreDf.head()

scoreDf.tail()

"""**SALE PRICE IS NOT USED TO CREATE THE SCORE FUNCTION**"""

X_train, X_test, Y_train, Y_test = train_test_split(scoreDf.iloc[:,:-2], scoreDf.iloc[:,-1], test_size=0.2, random_state=42)

lr = LinearRegression( )
lr.fit(X_train, Y_train)

print (scoreDf.columns)
print (lr.coef_, lr.intercept_)

"""**As we can see, Neighborhood embedding that we designed has the most weight followed by YearRemodAdd, YearBuilt and GrLivArea and BsmtSf**
**The Bedroom gets a negative weight which is a bit surprising. The Kitchen gets the least weight as expected.**
"""

lr.score(X_test, Y_test)

rr = Ridge(alpha=1000)
rr.fit(X_train, Y_train)

print (scoreDf.columns)
print (rr.coef_, rr.intercept_)

"""**As we can see, Neighborhood embedding that we designed has the most weight followed by YearRemodAdd, YearBuilt and GrLivArea and BsmtSf**
**The Bedroom gets a negative weight which is a bit surprising. The Kitchen gets the least weight as expected.**
"""

rr.score(X_test, Y_test)

Y_pred = rr.predict(X_test)
Y_pred[Y_pred>100.0] = 100.0
Y_pred[Y_pred<0.0] = 0.0

plt.figure(figsize=(10,5))
plt.grid(True)
plt.hist(Y_pred - Y_test, bins=40)
plt.title("Distribution of Err of the scoring function")
plt.xlabel("Y_Pred - Y_Test")
plt.show()

"""**The scoring function seems to have fairly performed well with the most error less that a magnitude of 10.**"""

pred  = pd.concat([X_test.reset_index(drop=True), Y_test.reset_index(drop=True), pd.DataFrame(Y_pred)], axis=1)

"""**PERFORMANCE OF THE DESIRABILITY SCORING FUNCTION**"""

pred.columns = pred.columns.tolist()[:-1] + ['Pred']  
pred = pred.sort_values(['score', 'Pred'], ascending=False)
pred.head(10)

"""**The houses with high scores seems to have gotten a high score mostly.**"""

pred.tail(10)

"""**The scoring function seems to be a bit off on the lower end of spectrum**

## Part 4 - Pairwise Distance Function

1. Distance metric: Given the params, it calculates the L1/L2/Cosine/Jaccard distance between the pairs of datapoints. 
2. I am extracting two SalePrice per square Feet 'features' to distinguish between a place in Manhattan and say, a place like Stony Brook. The difference between SalePrice per LotArea and SalePrice per GrLivArea will point that out.
3. I use the nearest house given by the KNN function that I train on. The KNN uses different distance measures as stated above.
"""

distCols = ['Neighborhood', 'YearBuilt', 'LotArea', 'GrLivArea', 'BedroomAbvGr', 'KitchenAbvGr', 'SalePrice']
distDf = pd.concat([dataObj[['Neighborhood']], data[distCols[1:]]], axis=1)
distDf['SalePerSF1'] = distDf['SalePrice']/distDf['LotArea']
distDf['SalePerSF2'] = distDf['SalePrice']/distDf['GrLivArea']

"""A train test split is created since if the train data is used for all the three distances, it results in the same KNN. (It will definitely have a match with itself.)"""

testSet = list(np.random.choice(distDf.index.tolist(), size=(int)(0.2*len(distDf)), replace=False ))
trainSet = list(set(distDf.index.tolist()) - set(testSet))
X_train, X_test = distDf.loc[trainSet], distDf.loc[testSet]

ind = [0,0,0]
dist = [0,0,0]
distances = ['l1', 'l2', 'cosine']
for i in range(len(distances)):
  neigh = NearestNeighbors(n_neighbors=5, n_jobs=-1, metric=distances[i])
  neigh.fit(X_train.iloc[:,1:])
  #neigh.fit(distDf.iloc[:,1:])
  dist[i], ind[i] = neigh.kneighbors(X_test.iloc[:, 1:])
  print (dist[i].shape, ind[i].shape)

print ('l1 and l2 agree on closest in %d out of %d rows'%(np.sum(ind[0][:,0] == ind[1][:,0]), X_test.shape[0]))
print ('l1 and cos agree on closest in %d out of %d rows'%(np.sum(ind[0][:,0] == ind[2][:,0]), X_test.shape[0]))
print ('cos and l2 agree on closest in %d out of %d rows'%(np.sum(ind[2][:,0] == ind[1][:,0]), X_test.shape[0]))

X_test.iloc[0:1]

"""A sample from the test data."""

X_train.iloc[ind[0][0]]

"""Output of the KNN for the data above using l1 dist"""

X_train.iloc[ind[1][0]]

"""Output of KNN for the data above using l2 dist"""

X_train.iloc[ind[2][0]]

"""Output for the data above using cosine distance (similarity).

**EVALUATION OF DISTANCE**
- Distribution of difference between the actual value in the data and the property suggested to be the closest to it for a set of columns.
"""

for j in range(X_test.shape[1]):
  err0, err1, err2 = [], [], []
  fig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(10,3))
  for i in range(len(X_test)):
    err0.append(X_test.iloc[i,j] - X_train.iloc[ind[0][i][0]][j])
    err1.append(X_test.iloc[i,j] - X_train.iloc[ind[1][i][0]][j])
    err2.append(X_test.iloc[i,j] - X_train.iloc[ind[2][i][0]][j])
  err = [err0, err1, err2]
  for idx,a in enumerate(ax):
    a.hist(err[idx])
    a.set_xlabel('actual(%s) - pred(%s)'%(X_test.columns[j],X_test.columns[j]))
    a.set_ylabel('#')
    a.set_title('Performance of %s'%(distances[int(idx)]))
    plt.tight_layout()

"""**Inference: Going by the performance of cosine distance in Neighborhood, GrLivArea, BedroomAbvGr, SalePrice, SalePeSF1 and SalePerSF2, I would definitely go with cosine distance!**

## Part 5 - Clustering

- **There is a direct linear connection between cosine distance and Eucledian distance for normalized vectors. Credits: [refer here](https://stats.stackexchange.com/questions/299013/cosine-distance-as-similarity-measure-in-kmeans).** 

- **I use K Means and agglomerative clustering techniques.**
"""

X_Norm = preprocessing.normalize(distDf.iloc[:,1:])
km = []
agg = []
for i in range(5,21,5):
  km.append(cluster.KMeans(n_clusters=i, n_jobs=-1).fit(X_Norm))
  agg.append(cluster.AgglomerativeClustering(n_clusters=i).fit(X_Norm))

"""**PERFORMANCE EVALUATION OF CLUSTERING ALGORITHM**

- Each of the cluster given by the clustering algorithm is taken and the actual Neighborhood of those points are plotted in a distribution plot. 

- In higher cluster space, when the all the actual neighborhood fall in the same bin of the new cluster value, it is a good clustering algorithm.
"""

for i in range(4):
  print ('-----------------------------------------------')
  print ('PERFORMANCE OF CLUSTERING WITH %d CLUSTERS - KMEANS'%((i+1)*5))
  clusterResKM = pd.concat([distDf, pd.DataFrame(km[i].labels_)], axis=1)
  clusterResKM.columns = list(clusterResKM.columns[:-1]) + ['labels']
  fig, ax = plt.subplots(nrows = (i+1), ncols = 5, figsize=(15,(i)+5))
  ax = ax.ravel()
  for j, a in enumerate(ax):
    a.hist(clusterResKM[clusterResKM.labels==int(j)].Neighborhood.values)
    a.set_xlabel("Cluster Result: %d"%j)
    a.set_ylabel("#")
    a.set_title("Dist. of actual Neighborhood")
  plt.tight_layout()
  plt.show()

"""**Inference: As the number of clusters increase the actual neighborhood start falling into one particular bin. This is evident from the above graph.**"""

for i in range(4):
  print ('-----------------------------------------------')
  print ('PERFORMANCE OF CLUSTERING WITH %d CLUSTERS - AGGLOMERATIVE'%((i+1)*5))  
  clusterResAG = pd.concat([distDf, pd.DataFrame(agg[i].labels_)], axis=1)
  clusterResAG.columns = list(clusterResAG.columns[:-1]) + ['labels']
  fig, ax = plt.subplots(nrows = (i+1), ncols = 5, figsize=(15,(i)+5))
  ax = ax.ravel()
  for j, a in enumerate(ax):
    a.hist(clusterResAG[clusterResAG.labels==int(j)].Neighborhood.values)
    a.set_xlabel("Cluster Result: %d"%j)
    a.set_ylabel("#")
    a.set_title("Dist. of actual Neighborhood")
  plt.tight_layout()
  plt.show()

"""**Inference: As the number of clusters increase the actual neighborhood start falling into one particular bin. This is evident from the above graph.**

## Part 6 - Linear Regression
"""

data.YearRemodAdd.describe()

data['SalePerSF1'] = data['SalePrice']/data['LotArea']
data['SalePerSF2'] = data['SalePrice']/data['GrLivArea']
sortedIndexSF1 = pd.concat([data.groupby('Neighborhood').SalePerSF1.mean(), data.groupby('Neighborhood').SalePerSF1.std()], axis=1)
sortedIndexSF1.columns = list(['meanVal', 'stdVal'])
sortedIndexSF1 = sortedIndexSF1.sort_values(['stdVal'])
sortedIndexSF1 = sortedIndexSF1.sort_values(['meanVal'], ascending=False)
sortedIndexSF1.reset_index(inplace=True)


sortedIndexSF2 = pd.concat([data.groupby('Neighborhood').SalePerSF2.mean(), data.groupby('Neighborhood').SalePerSF2.std()], axis=1)
sortedIndexSF2.columns = list(['meanVal', 'stdVal'])
sortedIndexSF2 = sortedIndexSF2.sort_values(['stdVal'])
sortedIndexSF2 = sortedIndexSF2.sort_values(['meanVal'], ascending=False)
sortedIndexSF2.reset_index(inplace=True)

sortedIndexSF1

sortedIndexSF2

for i in data.Neighborhood.unique():
  data.loc[ data.Neighborhood==i,'SalePerSF1'] = float(sortedIndexSF1[sortedIndexSF1.Neighborhood==i]['meanVal'])
  data.loc[data.Neighborhood==i,'SalePerSF2'] = float(sortedIndexSF2[sortedIndexSF2.Neighborhood==i]['meanVal'])

def lregress(X,Y):
  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)
  lr = LinearRegression(n_jobs=-1)
  lr.fit(X_train, Y_train)
  print ('\n------------------------------------\n LINEAR REGRESSION')
  pprint ((dict(zip(lrCols, lr.coef_))))
  Y_pred = lr.predict(X_test)
  print ("\nr2 score: ", metrics.r2_score(Y_test, Y_pred))
  print ("MAE: ",metrics.mean_absolute_error(Y_test, Y_pred))
  print ("RMSE: ",np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))
  print ("MAPE: ",np.mean((np.abs(Y_pred-Y_test))*100.0/(Y_test)))

def rregress(X, Y, alpha=1000):
  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)
  rr = Ridge(alpha=alpha)
  rr.fit(X_train, Y_train)
  Y_pred = rr.predict(X_test)
  print ('\n------------------------------------\n RIDGE REGRESSION')
  pprint ((dict(zip(lrCols, rr.coef_))))
  print ("\nr2 score: ", metrics.r2_score(Y_test, Y_pred))
  print ("MAE: ",metrics.mean_absolute_error(Y_test, Y_pred))
  print ("RMSE: ",np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))
  print ("MAPE: ",np.mean((np.abs(Y_pred-Y_test))*100.0/(Y_test)))

lrCols = ['Neighborhood', 'HouseStyle', 'YearBuilt', 'LotArea', 'GrLivArea', 'BedroomAbvGr', 'KitchenAbvGr', 'YearBuilt', 'YearRemodAdd',
          'GarageArea', 'OverallQual', 'SalePerSF1', 'SalePerSF2','SalePrice']
X, Y = pd.concat([dataObj[lrCols[:2]], data[lrCols[2:-1]]], axis=1), (data['SalePrice'])
lregress(X,Y)
rregress(X,Y)

"""- **Inference: An r2 score of 0.8 is good. From the set of input columns, OverallQual, Neighborhood, SalePerSf of GrLivArea, YearBuilt, YearRemodAdd, GarageArea and GrLivArea have positive weights**

- **Lot Area has negligible contribution and Bedroom, kitchen, housestyle have negative weights.**
"""

lrCols = ['Neighborhood', 'YearBuilt', 'LotArea', 'GrLivArea', 'BedroomAbvGr', 'KitchenAbvGr', 'YearBuilt', 'YearRemodAdd',
          'GarageArea', 'OverallQual', 'Fireplaces', 'TotalBsmtSF', 'TotRmsAbvGrd']
X, Y = pd.concat([dataObj['Neighborhood'], data[lrCols[1:]]], axis=1), data['SalePrice']
lregress(X, Y)
rregress(X, Y)

X.shape

"""## Part 7 - External Dataset

[Data](https://fred.stlouisfed.org/series/ATNHPIUS11180Q) from an external source specifies the Pricing Index of houses at Ames, IA over the years.
"""

# Commented out IPython magic to ensure Python compatibility.
lrCols = ['Neighborhood', 'HouseStyle', 'YearBuilt', 'LotArea', 'GrLivArea', 'BedroomAbvGr', 'KitchenAbvGr', 'YearBuilt', 'YearRemodAdd',
          'GarageArea', 'OverallQual', 'SalePerSF1', 'SalePerSF2','SalePrice']
X = pd.concat([dataObj[lrCols[:2]], data[lrCols[2:]]], axis=1)
# %ls

externalData = pd.read_csv('./HousePriceIndex.csv', parse_dates=['DATE'])
externalData.head()

"""It is merged with YearRemodAdd as the key, since the data is available only since 1986. The YearBuilt starts much before."""

externalData.columns = ['Date', 'PriceIndex']
externalData['YearRemodAdd'] = externalData.Date.dt.year

externalData = pd.DataFrame(externalData.groupby('YearRemodAdd').PriceIndex.mean())

externalData.reset_index()

mergedData = pd.merge(X, externalData, on='YearRemodAdd')
mergedData.shape

mergedData.columns

"""**YearRemodAdd was removed since PriceIndex was added based on that.**"""

lrCols = ['Neighborhood', 'HouseStyle', 'YearBuilt', 'LotArea', 'GrLivArea', 'BedroomAbvGr', 'KitchenAbvGr', 'PriceIndex',
          'GarageArea', 'OverallQual' ]
Y = mergedData[lrCols]
X = mergedData[['SalePrice']]
lregress(X,Y)
rregress(X,Y)

"""**Inference: The performance takes a sharp dip. The PriceIndex couldn't clearly convey the information to the model that YearRemodAdd could.**

## Part 8 - Permutation Test
"""

def RMSEVal(Y_test ,Y_pred):
  return np.sqrt(np.mean((np.log(Y_test) - np.log(Y_pred))**2))

PtestCols = ['Neighborhood', 'HouseStyle', 'YearBuilt', 'GrLivArea', 'BedroomAbvGr', 'KitchenAbvGr', 'GarageArea', 
             'OverallQual', 'SalePerSF1', 'SalePerSF2', 'OverallCond']
X, Y = pd.concat([dataObj[PtestCols[:2]], data[PtestCols[2:]]], axis=1), data['SalePrice']          
for i in PtestCols:
  
  Y = Y[~pd.isna(X[i])]
  x = X[~pd.isna(X[i])] 
  lr = LinearRegression()
  RMSLEScore = metrics.make_scorer(RMSEVal,greater_is_better=False)
  score, permutation_scores, pvalue = permutation_test_score(lr, x, Y, n_permutations = 100, 
                                                             scoring = RMSLEScore, cv =None)
  print (i, pvalue)

"""**Not clearly able to see why the p val for all the columns returns the same value. They are all lesser than 0.05. If this is true, these columns are suited for modeling.**

## Part 9 - Result
"""

def etregress(X, Y, n_est=80):
  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)
  et = ExtraTreesRegressor(n_estimators=n_est, criterion='mae', n_jobs=-1)
  et.fit(X_train, Y_train)
  Y_pred = et.predict(X_test)
  print ('\n------------------------------------\n EXTRA TREES REGRESSION')
  print ("\nr2 score: ", metrics.r2_score(Y_test, Y_pred))
  print ("MAE: ",metrics.mean_absolute_error(Y_test, Y_pred))
  print ("RMSE: ",np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))
  print ("MAPE: ",np.mean((np.abs(Y_pred-Y_test))*100.0/(Y_test)))  
  return et

lrCols = ['Neighborhood', 'HouseStyle', 'ExterQual', 'BsmtQual', 'KitchenQual', 'Foundation', 'YearBuilt', 'LotArea', 'GrLivArea', 'BedroomAbvGr', 'KitchenAbvGr', 'YearBuilt', 'YearRemodAdd',
          'GarageArea', 'OverallQual', 'Fireplaces', 'TotalBsmtSF', 'SalePerSF1', 'SalePerSF2', 'TotRmsAbvGrd']
X, Y = pd.concat([dataObj[lrCols[:6]], data[lrCols[6:]]], axis=1), data['SalePrice']
et = etregress(X, Y, 20)

ls

sampleFile = pd.read_csv('./test.csv', low_memory=False)
sampleFile = sampleFile[['Id']+lrCols]

sampleFile.head()

sampleFile.Neighborhood.replace(dict(zip(sortedIndexN.Neighborhood, reversed(range(1,len(sortedIndexN)+1)))), inplace=True)
sampleFile.head()

sampleFile.Neighborhood =  sampleFile.Neighborhood.replace(dict(zip(sortedIndexN.Neighborhood, reversed(range(1,len(sortedIndexN)+1)))))
sampleFile.head()

for i in sampleFile.columns:
  print (i, np.sum(sampleFile[i].isna()))

sampleFile.TotalBsmtSF = sampleFile.TotalBsmtSF.fillna(0)
sampleFile.GarageArea = sampleFile.GarageArea.fillna(0)

for i in sampleFile.columns:
  print (i, np.sum(sampleFile[i].isna()))

sampleFile.head()

X.columns

samplePred = et.predict(sampleFile.iloc[:,1:])

samplePred = pd.concat([sampleFile['Id'], pd.DataFrame(samplePred)], axis=1)
samplePred.columns = ['Id', 'SalePrice']
samplePred.head()

samplePred.to_csv("sample_submission1.csv", index=False)

lasso = GradientBoostingRegressor(n_estimators=100)
X_train, X_test, Y_train, Y_test = train_test_split((X), Y, test_size=0.20, random_state=42)
lasso.fit(X_train, Y_train)
Y_pred = lasso.predict(X_test)
print ('\n------------------------------------\n EXTRA TREES REGRESSION')
print ("\nr2 score: ", metrics.r2_score(Y_test, Y_pred))
print ("MAE: ",metrics.mean_absolute_error(Y_test, Y_pred))
print ("RMSE: ",np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))
print ("MAPE: ",np.mean((np.abs(Y_pred-Y_test))*100.0/(Y_test)))

lasso.feature_importances_

X.columns

"""**DETAILS**
- KAGGLE LINK: https://www.kaggle.com/adithya8
- HIGHEST RANK: 3417
- SCORE: 0.166683
- NUMBER OF ENTRIES: 1
- IMAGE: [here](https://drive.google.com/file/d/16Fe-LMI62MbpdDBugG85d88sDog-uWLp/view?usp=sharing)
"""

